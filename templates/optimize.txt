[TASK]: <One‑sentence description of what to optimize (e.g. “Improve response time of search endpoint” or “Refactor image-processing loop for efficiency”)>
[MODE]: Optimize
[PROJECT CONTEXT]: <One line or “see README.md”>
[BRANCH]: <e.g. optimize/xyz or main>
[YOLO_MODE]: on      # “on” = perform full optimization cycle, “off” = present plan before coding

---

### 1. Profile (Benchmark & Analysis)
- Restate goal: “I will optimize…”
- **Baseline measurement**: run `<benchmark_command>` (e.g. `ab`, `pytest-benchmark`, custom script)  
- **Collect metrics**: note key stats (latency, memory, CPU usage)  
- **Short plan** (≤3 bullets):  
  1. Identify hotspots via profiler or logs  
  2. Select candidate functions/modules for refactoring  
  3. Define performance targets (e.g. “reduce response time by 30%”)

> If YOLO_MODE=off, present this profile summary and await approval; else proceed.

---

### 2. Identify Hotspots
- **Use profiler**: execute `<profile_command>` (e.g. `cProfile`, `perf`, Chrome DevTools)  
- **Analyze output**: find top N slowest functions or memory‑heavy operations  
- **Document findings**: list each hotspot with current metrics  

---

### 3. Refactor & Optimize
For each hotspot:
1. **Locate code**: use `search_code` + `read_file`  
2. **Implement optimization**: use `edit_file` to  
   - Improve algorithmic complexity  
   - Apply caching, batching, concurrency, or native libraries  
   - Remove unnecessary allocations or expensive calls  
3. **Commit checkpoint** (optional): group related optimizations  

> Follow “Read Before Write.” Focus on correctness first, then micro‑optimizations.

---

### 4. Validate (Re‑benchmark & Test)
- **Re-run benchmarks**: `<benchmark_command>` and compare to baseline  
- **Run tests**: `<test_command>` to ensure behavior unchanged  
- **Check regressions**: verify no new errors or performance pitfalls  
- **If targets not met**: refine optimizations and repeat benchmarks

> Loop until performance targets are satisfied or no further safe gains are possible.

---

### 5. Report (Summary)
In chat, provide:
- Baseline vs. final metrics (e.g. “Latency reduced from 200 ms to 120 ms (40% improvement)”)  
- List of optimizations applied (file/function + strategy)  
- Confirmation that all tests pass and no regressions introduced  

> Keep it concise—highlight gains and key changes only.

---

### 6. Context Update (Persistence)
- **context.json**: update with  
  ```json
  {
    "last_task": "<TASK>",
    "summary": "Optimized <component>; performance improved by X%.",
    "metrics": { "before": {…}, "after": {…} },
    "next_steps": [ ... ]
  }
